\documentclass[a4paper]{llncs}
\usepackage{llncsdoc}

%% Deutsche Anpassungen %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage[ngerman]{babel}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}

% Paket für Graphiken
\usepackage[pdftex]{graphicx}

%\Paket für Hyperlinks
\usepackage[
bookmarks=true,
bookmarksopen=true,
bookmarksnumbered=true,
breaklinks=true,
colorlinks=true,
linkcolor=black,
anchorcolor=black,
citecolor=black,
filecolor=black,
menucolor=black,
pagecolor=black,
urlcolor=black
]{hyperref}

\begin{document}

\title{Clustering of DBPedia Subjects}
\subtitle{Seminar Map/Reduce Algorithms on Hadoop}
\author{Robert Pfeiffer, Tobias Schmidt}
\institute{Fachgebiet Informationssysteme\\Hasso-Plattner-Institut für Softwaresystemtechnik\\Prof.-Dr.-Helmert-Str. 2-3\\14482 Potsdam, Deutschland\\31. August 2009}

\maketitle

\begin{abstract}
TODO\\
wenn alles fertig ist\\
werden es mal 3 Zeilen?
\end{abstract}

\section{Einleitung}
Mit der fortschreitenden Entwicklung der Computertechnik steigt auch das zu bearbeitende Datenvolumen stetig an. Damit auf immer größeren Datenmengen weiterhin effizient Berechnungen ausgeführt werden können, wurde von Google\footnote{\url{http://www.google.com/}} ein Framework namens MapReduce entwickelt. Mit diesem Programmrahmen ist es möglich, Berechnungen auf mehere Rechner zu verteilen und parallel ablaufen zu lassen.

Im Seminar \emph{Map/Reduce Algorithms on Hadoop}\footnote{\url{http://www.hpi.uni-potsdam.de/naumann/lehre/ss_09/mapreduce_algorithms_on_hadoop.html}} haben wir uns mit dem, in Java implementierem, MapReduce-Framework \emph{Hadoop}\footnote{\url{http://hadoop.apache.org/}} auseinandergesetzt und einen Algorithmus implementiert, der eine Menge von Objekten anhand ihrer Ähnlichkeit gruppiert.

\subsection{MapReduce}
Der MapReduce Ansatz beinhaltet allgemein zwei Schritte: den Map- und den Reduce-Schritt. Während des Map-Schrittes wird ein Problem in mehrere Teilprobleme zerlegt, die idealerweise parallel und unabhängig voneinander ausgeführt werden. Anschließend werden die Ergebnisse der Teilprobleme im Reduce-Schritt wieder zu einem Gesamtergebnis zusammengesetzt.

\subsection{Die Daten}
Die Datenmenge, die uns zur Verfügung gestellt wurde, sind Objekte aus der \emph{DBPedia}\footnote{\url{http://dbpedia.org/}}. Das \emph{DBPedia}-Projekt extrahiert Informationen aus der Online-Enzyklopädie \emph{Wikipedia}\footnote{\url{http://www.wikipedia.org/}}, bereitet diese auf und stellt diese strukturiert zur freien Verfügung. Einträge der \emph{DBPedia} entsprechen damit weitesgehend Artikeln der \emph{Wikipedia}.

Als Beispiel für Informationen in der DBPedia sind die Infoboxen zu nennen, die in vielen Artikeln wie z.B. \emph{Europa}\footnote{\url{http://de.wikipedia.org/wiki/Europa}} existieren.

\subsection{Clustering}
Algorithmen beziehungsweise Verfahren, die eine Menge von Objekten zu Gruppen (Cluster) mit jeweils ähnlichen Eigenschaften zusammenfasst, bezeichnet man als Clusteranalyseverfahren. Es gibt eine ganze Reihe von unterschiedlichen Ansätzen zur Analyse, die alle unterschiedliche Vor- und Nachteile haben. Grundsätzlich unterscheidet man zwischen partitionierenden und hierarischen Analyseverfahren.

Für viele Clusterverfahren wird ein Abstandsmaß benötigt. Dieses gibt für zwei Objekte einen Wert zurück, der angibt, wie weit die beiden Objekte voneinander entfernt sind. Kleinere Werte beschreiben üblicherweise geringere Abstände. Es gibt verschiedene Abstandsmaße, die bei unterschiedlichen Daten unterschiedlich gute Ergebnisse liefern. Als Beispiele sind das \emph{Euklidische Abstandsmaß} und die \emph{Jaccard-Distance} zu nennen. 

\subsection{k-Means}
Wir haben uns für das partitionierende Clusteranalyseverfahren \emph{k-Means}\footnote{\url{http://de.wikipedia.org/wiki/K-Means-Algorithmus}} entschieden. \emph{k-Means} zeichnet sich durch seine hohe Geschwindigkeit aus, liefert jedoch nicht zwingend die optimale Lösung. Desweiteren muss man bei \emph{k-Means} die Anzahl der gewünschten Cluster im Vorfeld vorgeben.

Zu Beginn des Algorithmus wählt man zufällig die gewünschte Anzahl von Clustern aus der Objektmenge aus. Diese Objekte bilden die initialen Clusterzentren. Aus der zufälligen Auswahl der Clusterzentren ergibt sich die Konsequenz, dass bei verschiedenen Programmdurchläufen mit gleichen Eingabedateien, unterschiedliche Cluster berechnet werden können. Nach der Wahl der Clusterzentren, beginnt der iterative Teil des Algorithmus.

Für jedes Objekt wird mit Hilfe des Abstandsmaßes der Abstand zu jedem Clusterzentrum berechnet. Anschließend wird das Objekt demjenigen Cluster zugerordnet, zu dessem Clusterzentrum es den geringsten Abstand aufweist. Nach dem alle Objekte einem Cluster zugeordnet worden sind, wird das neue Clusterzentrum bestimmt, in dem der Schwerpunkt aller dem Cluster zugeordneten Objekte berechnet wird.

Dieser Teil wird solange wiederholt, bis nach einer Iteration kein Objekt mehr einem anderen Clusterzentrum zugeordnet wird. Alternativ kann auch eine prozentuale Schranke festlegen (zum Beispiel: wiederhole bis weniger als 2\% der Objekte einem anderen Cluster zugeordnet werden).

\section{Implementierung}

\subsection{k-means Algorithmus}
- 2 Phasen. Welche Daten werden von Mapper an Reducer und umgekehrt übergeben\\
- Vorteile / Nachteile gegenüber anderen Ansätzen\\
- <überarbeitete Grafik ?>

\subsection{Distributed Cache}
- Problem: Verteilung der Centroids\\
- Lösung mittels Distributed Cache\\
- Vorteile: in Hadoop, synchrone Datenhaltung, wenig Overhead für die restlichen berechnungen\\
- Nachteile: bricht das Map/Reduce Konzept\\
- andere Möglichkeit: kartesisches Produkt

\subsection{Sequence Files}
- Problem: Daten sind große Matrizen\\
- Vorteile:\\
    - Hadoop Format, dadurch gibt es bereits ein Interface und Hadoop kann den Input splitten\\
    - komprimiert\\
- Nachteile:\\
    - kodiertes Format, daher für Menschen nicht lesbar\\
    - keine Information über die Gesamtgröße

\subsection{Festkommazahlen}
- Centroids werden durch Vektor über Festkommazahlen mit der Größe eines Bytes dargestellt\\
- Vorteile: klein Datengröße\\
- Nachteile: evt. zu grobe Abstufungen, man muss immer an die Umrechnung denken, unflexibel, schlecht austauschbar

\section{Evaluierung}
Eingabegröße
Anzahl der Cluster
Abbruch

\section{Fazit}
TODO

\begin{thebibliography}{--------}

\bibitem[KI2008]{KI2008}
  Toby Segaran.
  {\em Kollektive Intelligenz}.
  O'Reilly, 2008
   
\end{thebibliography}

\end{document}
