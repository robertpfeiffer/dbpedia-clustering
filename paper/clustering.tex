%&program=pdflatex
%&encoding=UTF-8 Unicode
\documentclass[a4paper]{llncs}
\usepackage{llncsdoc}

%% Deutsche Anpassungen %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage[ngerman]{babel}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}

% Paket für Graphiken
\usepackage[pdftex]{graphicx}

%\Paket für Hyperlinks
\usepackage[
bookmarks=true,
bookmarksopen=true,
bookmarksnumbered=true,
breaklinks=true,
colorlinks=true,
linkcolor=black,
anchorcolor=black,
citecolor=black,
filecolor=black,
menucolor=black,
pagecolor=black,
urlcolor=black
]{hyperref}

\begin{document}

\title{Clustering of DBPedia Subjects}
\subtitle{Seminar Map/Reduce Algorithms on Hadoop}
\author{Robert Pfeiffer, Tobias Schmidt}
\institute{Fachgebiet Informationssysteme\\Hasso-Plattner-Institut für Softwaresystemtechnik\\Prof.-Dr.-Helmert-Str. 2-3\\14482 Potsdam, Deutschland\\31. August 2009}

\maketitle

\begin{abstract}
TODO\\
wenn alles fertig ist\\
werden es mal 3 Zeilen?
\end{abstract}

\section{Einleitung}
Mit der fortschreitenden Entwicklung der Computertechnik steigt auch das zu bearbeitende Datenvolumen stetig an. Damit auf immer größeren Datenmengen weiterhin effizient Berechnungen ausgeführt werden können, wurde von Google\footnote{\url{http://www.google.com/}} ein Framework namens MapReduce entwickelt. Mit diesem Programmrahmen ist es möglich, Berechnungen auf mehere Rechner zu verteilen und parallel ablaufen zu lassen.

Im Seminar \emph{Map/Reduce Algorithms on Hadoop}\footnote{\url{http://www.hpi.uni-potsdam.de/naumann/lehre/ss_09/mapreduce_algorithms_on_hadoop.html}} haben wir uns mit dem, in Java implementierem, MapReduce-Framework \emph{Hadoop}\footnote{\url{http://hadoop.apache.org/}} auseinandergesetzt und einen Algorithmus implementiert, der eine Menge von Objekten anhand ihrer Ähnlichkeit gruppiert.

\section{Grundlagen}

\subsection{MapReduce}
MapReduce ist ein Programmiermodell, welches erstmals 2004 vorgestellt wurde \cite{DG04}.
Der MapReduce Ansatz beinhaltet allgemein zwei Funktionen: die Map- und die Reduce-Funktion. In der Map-Funktion wird ein Problem in mehrere Teilprobleme zerlegt, die idealerweise parallel und unabhängig voneinander ausgeführt werden. Anschließend werden die Ergebnisse der Teilprobleme in der Reduce-Funktion wieder zu einem Gesamtergebnis zusammengesetzt.

Die Map-Funktion erwartet als Eingabe Schlüssel-Wert-Paare und gibt neue Schlüssel-Wert-Paare als Ergebnis aus. Die Reduce-Funktion erhält als Eingabe einen Schlüssel und alle dazugehörigen Werte. Diese Werte werden je nach vorgegebener Berechnung zusammengeführt und dem Schlüssel zugeordnet. Die Reduce gibt anschließend wiederum Schlüssel-Wert-Paare aus.

\subsection{Hadoop TODO}
Das Hadoop Framework übernimmt die Zerteilung der Eingabedatei und generiert aus den einzelnen Teilen sogenannte Maptasks. Die einzelnen Rechner im Hadoop-Cluster bearbeiten anschließend diese Maptasks. Für jedes Schlüssel-Wert-Paar wird die Map-Funktion aufgerufen.

\subsection{Die Daten}
Die Datenmenge, die uns zur Verfügung gestellt wurde, sind Objekte aus der \emph{DBPedia}\footnote{\url{http://dbpedia.org/}}. Das \emph{DBPedia}-Projekt extrahiert Informationen aus der Online-Enzyklopädie \emph{Wikipedia}\footnote{\url{http://www.wikipedia.org/}}, bereitet diese auf und stellt diese strukturiert zur freien Verfügung. Einträge der \emph{DBPedia} entsprechen damit weitesgehend Artikeln der \emph{Wikipedia}.

Als Beispiel für Informationen in der DBPedia sind die Infoboxen zu nennen, die in vielen Artikeln wie z.B. \emph{Europa}\footnote{\url{http://de.wikipedia.org/wiki/Europa}} auf der rechten Seite existieren.

\subsection{Clustering}
Algorithmen beziehungsweise Verfahren, die eine Menge von Objekten zu Gruppen (Cluster) mit jeweils ähnlichen Eigenschaften zusammenfassen, bezeichnet man als Clusteranalyseverfahren. Es gibt eine ganze Reihe von unterschiedlichen Ansätzen zur Analyse, die alle unterschiedliche Vor- und Nachteile haben. Grundsätzlich unterscheidet man zwischen partitionierenden und hierarischen Analyseverfahren.

Für viele Clusterverfahren wird ein Abstandsmaß benötigt. Dieses gibt für zwei Objekte einen Wert zurück, der angibt, wie weit die beiden Objekte voneinander entfernt sind. Kleinere Werte beschreiben üblicherweise geringere Abstände. Es gibt verschiedene Abstandsmaße, die für unterschiedlichen Daten unterschiedlich gute Ergebnisse liefern. Als Beispiele sind das \emph{Euklidische Abstandsmaß} und die \emph{Jaccard-Distance} zu nennen.

\subsection{k-Means}
Wir haben uns für das partitionierende Clusteranalyseverfahren \emph{k-Means}\footnote{\url{http://de.wikipedia.org/wiki/K-Means-Algorithmus}} entschieden. \emph{k-Means} zeichnet sich durch seine hohe Geschwindigkeit aus, liefert jedoch nicht zwingend die optimale Lösung. Desweiteren muss man bei \emph{k-Means} die Anzahl der gewünschten Cluster im Vorfeld vorgeben.

Zu Beginn des Algorithmus wählt man zufällig die gewünschte Anzahl von Clustern aus der Objektmenge aus. Diese Objekte bilden die initialen Clusterzentren. Aus der zufälligen Auswahl der Clusterzentren ergibt sich die Konsequenz, dass bei verschiedenen Programmdurchläufen mit gleichen Eingabedateien, unterschiedliche Cluster berechnet werden können. Nach der Wahl der Clusterzentren, beginnt der iterative Teil des Algorithmus.

Für jedes Objekt wird mit Hilfe des Abstandsmaßes der Abstand zu jedem Clusterzentrum berechnet. Anschließend wird das Objekt demjenigen Cluster zugerordnet, zu dessem Clusterzentrum es den geringsten Abstand aufweist. Nach dem alle Objekte einem Cluster zugeordnet worden sind, wird das neue Clusterzentrum bestimmt, in dem der Schwerpunkt aller dem Cluster zugeordneten Objekte berechnet wird.

Dieser Vorgang wird solange wiederholt, bis nach einer Iteration kein Objekt mehr einem anderen Clusterzentrum zugeordnet wird. Alternativ kann auch eine prozentuale Schranke festgelegt werden (zum Beispiel: wiederhole bis weniger als 2\% der Objekte einem anderen Cluster zugeordnet werden).

\section{Implementation}

\subsection{Datenformat}
- Problem: Daten sind große Matrizen\\
- Vorteile:\\
    - Hadoop Format, dadurch gibt es bereits ein Interface und Hadoop kann den Input splitten\\
    - komprimiert\\
- Nachteile:\\
    - kodiertes Format, daher für Menschen nicht lesbar\\
    - keine Information über die Gesamtgröße

\subsection{Generierung der Clusterzentren}
Das erste Problem bei der Implementierung des k-Means Algorithmus stellte die Generierung der initialen Clusterzentren dar. Im Hadoop Framework werden eine Map- und eine Reduce-Funktion zu einem Job zusammengefasst und es ist möglich, in einem Programm mehrere Jobs hintereinander auszuführen. Dadurch bestand eine Möglichkeit darin, mittels eines Jobs die Zentren zu generieren und mit Hilfe eines zweiten Jobtyps den iterativen Teil des Algorithmus auszuführen. Die zweite von uns erarbeitete Idee war, die Clusterzentren vor dem Starten des Hadoop-Programmes seperat zu generieren und als weiteren Eingabeparameter mitzuliefern. Wir haben uns für das zweite Verfahren entschieden, da es zum einen zu diesem Zeitpunkt für uns leichter zu implementieren war und zum anderen durch die einmalige Generierung einen Geschwindigkeitsvorteil darstellte. Für ein finale Version des Programms wäre es jedoch von Vorteil, wenn der Benutzer nicht selbst die Clusterzentren generieren müsste.

\subsection{Distributed Cache}
- Problem: Verteilung der Centroids\\
- Lösung mittels Distributed Cache\\
- Vorteile: in Hadoop, synchrone Datenhaltung, wenig Overhead für die restlichen berechnungen\\
- Nachteile: bricht das Map/Reduce Konzept\\
- andere Möglichkeit: kartesisches Produkt

\subsection{Berechnung der Cluster}
Nach dem die Clusterzentren mittels des Distributed Cache auf den einzelnen Nodes verteilt wurden, beginnt die eigentliche Berechnung.
Der iterative Teil des k-Means-Algorithmus lässt sich relativ leicht auf das MapReduce-Schema abbilden. Für jedes Subjekt wird eine Map-Funktion aufgerufen. 
Diese berechnet den Abstand des Subjektes zu jedem Clusterzentrum, welche mittels Distributed Cache zur Verfügung stehen.

Für die Abstandsberechnung haben wir ein Interface \emph{Distance} eingeführt, welches unter anderem die Funktion \emph{between} bereitstellt.
Diese Funktion erwartet als Eingabe zwei Vektoren gleicher Länge und gibt eine reele Zahl zurück, welche den Abstand zwischen den beiden Vektoren repräsentiert.
Wir haben mit dem \emph{Euklidischen Abstandsmaß} und der \emph{Jaccard Distance} zwei verschiedene Implementationen des Interfaces erstellt.
Welche Implementation verwendet wird, ist dem Benutzer überlassen, der dies in der Konfigurationsdatei einstellen kann.

Das Resultat der Map-Funktion ist ein Schlüssel-Wert-Paar, bestehend aus dem Schlüssel des nächsten Centers und dem Vektor des Subjektes.

Die Reduce-Phase beginnt, sobald für jedes Subjekt das nächste Clusterzentrum berechnet wurde. ...

\subsection{Abbruch der Iteration}

\section{Evaluierung}
Eingabegröße
Anzahl der Cluster
Abbruch

\section{Fazit}
TODO

\begin{thebibliography}{------------}

\bibitem[KI2008]{KI2008}
  Segaran, Toby.
  {\em Kollektive Intelligenz}.
  O'Reilly, 2008

\bibitem[DG04]{DG04}
  Dean, Jeffrey; Ghemawat, Sanjay.\\
  {\em MapReduce: Simplified Data Processing on Large Clusters}.\\
  San Francisco, CA, 2004
   
\end{thebibliography}

\end{document}
