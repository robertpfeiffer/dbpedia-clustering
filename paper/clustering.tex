\documentclass[a4paper]{llncs}
\usepackage{llncsdoc}

%% Deutsche Anpassungen %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage[ngerman]{babel}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}

% Paket für Graphiken
\usepackage[pdftex]{graphicx}

%\Paket für Hyperlinks
\usepackage{hyperref}

\title{Clustering of DBPedia Subjects}
\subtitle{Seminar Map/Reduce Algorithms on Hadoop}
\author{Robert Pfeiffer, Tobias Schmidt}
\institute{Fachgebiet Informationssysteme\\Hasso-Plattner-Institut für Softwaresystemtechnik\\Prof.-Dr.-Helmert-Str. 2-3\\14482 Potsdam, Deutschland\\August 2009}
\begin{document}
\maketitle
\begin{abstract}
TODO\\
wenn alles fertig ist\\
werden es mal 3 Zeilen?
\end{abstract}

\section{Einleitung}
Mit der fortschreitenden Entwicklung der Computertechnik steigt auch zu bearbeitende Datenvolumen stetig an. Damit auch auf immer größeren Datenmengen weiterhin effizient Berechnungen ausgeführt werden können, wurde von Google Inc. ein Framework namens MapReduce entwickelt. Mit diesem Programmrahmen ist es möglich, Berechnungen auf mehere Rechner und CPUs zu verteilen und parallel ablaufen zu lassen.\\
Der MapReduce Ansatz beinhaltet allgemein zwei Schritte: den Map- und den Reduce-Schritt. Während des Map-Schrittes wird ein Problem in mehrere Teilprobleme zerlegt, die idialerweise parallel und unabhängig voneinander ausgeführt werden. Anschließend werden die Ergebnisse der Teilprobleme im Reduce-Schritt zu einem Gesamtergebnis zusammengesetzt.\\
Im Seminar \emph{Map/Reduce Algorithms on Hadoop}\footnote{\url{http://www.hpi.uni-potsdam.de/naumann/lehre/ss_09/mapreduce_algorithms_on_hadoop.html}} haben wir uns mit dem, in Java implementierem, MapReduce-Framework \emph{Hadoop}\footnote{\url{http://hadoop.apache.org/}} auseinandergesetzt und einen Algorithmus implementiert, der eine Menge von \emph{DBPedia}-Einträge anhand ihrer Ähnlichkeit gruppiert. Die \emph{DBPedia}\footnote{\url{http://dbpedia.org/}} ist eine Datenbank, die Informationen aus der Online-Enzyklopädie \emph{Wikipedia}\footnote{\url{http://www.wikipedia.org/}} extrahiert, aufbereitet und strukturiert zur freien Verfügung stellt. Einträge der \emph{DBPedia} entsprechen damit weitesgehend Artikeln der \emph{Wikipedia}.

\section{Clustering}
Algorithmen beziehungsweise Verfahren die eine Menge von Objekten zu Gruppen zusammenfasst, bezeichnet man als Clusteranalyseverfahren. Es gibt eine ganze Reihe von unterschiedlichen Ansätzen zur Analyse, die alle unterschiedliche Vor- und Nachteile haben. Wir haben uns für den \emph{k-Means-Algorithmus}\footnote{\url{http://de.wikipedia.org/wiki/K-Means-Algorithmus}} entschieden. \emph{k-Means} zeichnet sich durch seine Geschwindigkeit aus, liefert jedoch nicht zwingend die optimale Lösung.\\

\section{Implementierung}

\subsection{k-means Algorithmus}
- 2 Phasen. Welche Daten werden von Mapper an Reducer und umgekehrt übergeben\\
- Vorteile / Nachteile gegenüber anderen Ansätzen\\
- <überarbeitete Grafik ?>

\subsection{Distributed Cache}
- Problem: Verteilung der Centroids\\
- Lösung mittels Distributed Cache\\
- Vorteile: in Hadoop, synchrone Datenhaltung, wenig Overhead für die restlichen berechnungen\\
- Nachteile: bricht das Map/Reduce Konzept\\
- andere Möglichkeit: kartesisches Produkt

\subsection{Sequence Files}
- Problem: Daten sind große Matrizen\\
- Vorteile:\\
    - Hadoop Format, dadurch gibt es bereits ein Interface und Hadoop kann den Input splitten\\
    - komprimiert\\
- Nachteile:\\
    - kodiertes Format, daher für Menschen nicht lesbar\\
    - keine Information über die Gesamtgröße

\subsection{Festkommazahlen}
- Centroids werden durch Vektor über Festkommazahlen mit der Größe eines Bytes dargestellt\\
- Vorteile: klein Datengröße\\
- Nachteile: evt. zu grobe Abstufungen, man muss immer an die Umrechnung denken, unflexibel, schlecht austauschbar

\section{Evaluierung}
Eingabegröße
Anzahl der Cluster
Abbruch

\section{Fazit}
TODO

\begin{thebibliography}{--------}

\bibitem[KI2008]{KI2008}
  Toby Segaran.
  {\em Kollektive Intelligenz}.
  O'Reilly, 2008
   
\end{thebibliography}

\end{document}
